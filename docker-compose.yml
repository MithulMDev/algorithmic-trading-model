version: '3.8'

services:
  # Zookeeper - Required for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10
    volumes:
      - kafka-data:/var/lib/kafka/data

  # Redis - In-Memory Cache for Latest OHLCV Data
  redis:
    image: redis:7.2-alpine
    hostname: redis
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # RedisInsight - Official Redis GUI
  redis-insight:
    image: redis/redisinsight:latest
    container_name: redis-insight
    depends_on:
      - redis
    ports:
      - "5540:5540"
    networks:
      - kafka-network
    volumes:
      - redis-insight-data:/data
    environment:
      RIPORT: 5540
      RIHOST: 0.0.0.0

  # Python Producer Application
  producer:
    build:
      context: ./kafka-producer
      dockerfile: Dockerfile
    container_name: ohlcv-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      # MongoDB Configuration (accessing host machine)
      MONGO_URI: "mongodb://host.docker.internal:27017/"
      MONGO_DATABASE: "ohlcv_data"
      MONGO_COLLECTION: "combined_rows"
      MONGO_TIMEOUT_MS: "10000"
      MONGO_MAX_RETRIES: "10"
      MONGO_RETRY_DELAY_SEC: "5"
      
      # Kafka Configuration
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      KAFKA_TOPIC: "ohlcv-stream"
      KAFKA_NUM_PARTITIONS: "3"
      KAFKA_REPLICATION_FACTOR: "1"
      KAFKA_MAX_RETRIES: "10"
      KAFKA_RETRY_DELAY_SEC: "5"
      
      # Processing Configuration
      DELAY_BETWEEN_RECORDS: "1.0"
      PREFETCH_ALL_DATA: "true"
      
      # Logging Configuration
      LOG_DIR: "/app/logs"
      LOG_LEVEL: "INFO"
    networks:
      - kafka-network
    volumes:
      - ./logs:/app/logs
      - ./kafka-producer/kafka_producer.py:/app/kafka_producer.py
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: on-failure

  # Optional: Kafka UI for monitoring
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - kafka-network

  # Influxdb
  influxdb:
    image: influxdb:2.7-alpine
    container_name: influxdb
    ports:
      - "8086:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: password123
      DOCKER_INFLUXDB_INIT_ORG: my-org
      DOCKER_INFLUXDB_INIT_BUCKET: ohlcv_data
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: my-super-secret-auth-token
    networks:
      - kafka-network
    volumes:
      - influxdb-data:/var/lib/influxdb2
    healthcheck:
      test: ["CMD", "influx", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ClickHouse
  clickhouse:
    image: clickhouse/clickhouse-server:23.8-alpine
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
    networks:
      - kafka-network
    volumes:
      - clickhouse-data:/var/lib/clickhouse
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ClickHouse Web UI (Tabix)
  clickhouse-ui:
    image: spoonest/clickhouse-tabix-web-client:latest
    container_name: clickhouse-ui
    depends_on:
      - clickhouse
    ports:
      - "8088:80"
    networks:
      - kafka-network
    environment:
      CH_HOST: clickhouse
      CH_PORT: 8123
      CH_USER: default
      CH_PASS: ""

  # Spark Consumer Application
  spark-consumer:
    build:
      context: ./spark-consumer
      dockerfile: Dockerfile
    container_name: ohlcv-spark-consumer
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
      influxdb:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      producer:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      KAFKA_TOPIC: "ohlcv-stream"
      KAFKA_STARTING_OFFSETS: "earliest"
      KAFKA_MAX_OFFSETS_PER_TRIGGER: "1000"
      SPARK_APP_NAME: "OHLCV-Spark-Consumer"
      TRIGGER_INTERVAL: "500 milliseconds"
      CHECKPOINT_LOCATION: "/app/checkpoints"
      LOG_DIR: "/app/logs"
      DATA_LOG_DIR: "/app/logs/data"
      LOG_LEVEL: "INFO"
      # Redis Configuration
      REDIS_HOST: "redis"
      REDIS_PORT: "6379"
      REDIS_DB: "0"
      REDIS_SOCKET_TIMEOUT: "5"
      # InfluxDB Configuration
      INFLUX_URL: "http://influxdb:8086"
      INFLUX_TOKEN: "my-super-secret-auth-token"
      INFLUX_ORG: "my-org"
      INFLUX_BUCKET: "ohlcv_data"
      # ClickHouse Configuration
      CLICKHOUSE_HOST: "clickhouse"
      CLICKHOUSE_PORT: "8123"
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DATABASE: "default"
    networks:
      - kafka-network
    volumes:
      - ./logs/spark:/app/logs
      - ./spark-consumer/spark_consumer.py:/app/spark_consumer.py
      - spark-checkpoints:/app/checkpoints
    command: >
      spark-submit
      --master local[*]
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      /app/spark_consumer.py
    restart: on-failure

networks:
  kafka-network:
    driver: bridge

volumes:
  redis-insight-data:
    driver: local
  kafka-data:
    driver: local
  spark-checkpoints:
    driver: local
  # Need to comment this out - so that data is not stored locally and clears on restart or every compose up
  influxdb-data:
    driver: local
  clickhouse-data:
    driver: local
