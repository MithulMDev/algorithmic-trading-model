version: '3.8'

services:
  # Zookeeper - Required for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    volumes:
      - kafka-data:/var/lib/kafka/data
    restart: unless-stopped

  # Redis - In-Memory Cache for Latest OHLCV Data
  redis:
    image: redis:7.2-alpine
    hostname: redis
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru --appendonly yes
    networks:
      - kafka-network
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  # RedisInsight - Official Redis GUI
  redis-insight:
    image: redis/redisinsight:latest
    container_name: redis-insight
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "5540:5540"
    networks:
      - kafka-network
    volumes:
      - redis-insight-data:/data
    environment:
      RIPORT: 5540
      RIHOST: 0.0.0.0
    restart: unless-stopped

  # InfluxDB - Time Series Database
  influxdb:
    image: influxdb:2.7-alpine
    container_name: influxdb
    ports:
      - "8086:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: password123
      DOCKER_INFLUXDB_INIT_ORG: my-org
      DOCKER_INFLUXDB_INIT_BUCKET: ohlcv_data
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: my-super-secret-auth-token
    networks:
      - kafka-network
    volumes:
      - influxdb-data:/var/lib/influxdb2
    healthcheck:
      test: ["CMD", "influx", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ClickHouse - Analytics Database
  clickhouse:
    image: clickhouse/clickhouse-server:23.8-alpine
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
    networks:
      - kafka-network
    volumes:
      - clickhouse-data:/var/lib/clickhouse
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ClickHouse Web UI (Tabix)
  clickhouse-ui:
    image: spoonest/clickhouse-tabix-web-client:latest
    container_name: clickhouse-ui
    depends_on:
      clickhouse:
        condition: service_healthy
    ports:
      - "8088:80"
    networks:
      - kafka-network
    environment:
      CH_HOST: clickhouse
      CH_PORT: 8123
      CH_USER: default
      CH_PASS: ""
    restart: unless-stopped

  # Python Producer Application
  producer:
    build:
      context: ./kafka-producer
      dockerfile: Dockerfile
    container_name: ohlcv-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      # MongoDB Configuration (accessing host machine)
      MONGO_URI: "mongodb://host.docker.internal:27017/"
      MONGO_DATABASE: "ohlcv_data"
      MONGO_COLLECTION: "combined_rows"
      MONGO_TIMEOUT_MS: "10000"
      MONGO_MAX_RETRIES: "10"
      MONGO_RETRY_DELAY_SEC: "5"
      
      # Kafka Configuration
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      KAFKA_TOPIC: "ohlcv-stream"
      KAFKA_NUM_PARTITIONS: "3"
      KAFKA_REPLICATION_FACTOR: "1"
      KAFKA_MAX_RETRIES: "10"
      KAFKA_RETRY_DELAY_SEC: "5"
      
      # Processing Configuration
      DELAY_BETWEEN_RECORDS: "1.0"
      PREFETCH_ALL_DATA: "true"
      
      # Logging Configuration
      LOG_DIR: "/app/logs/kafka"
      LOG_LEVEL: "INFO"
    networks:
      - kafka-network
    volumes:
      - ./logs/kafka:/app/logs
      - ./kafka-producer/kafka_producer.py:/app/kafka_producer.py
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Optional: Kafka UI for monitoring
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - kafka-network
    restart: unless-stopped

  # ===================================================================
  # OHLCV PROCESSING SERVICES (3 separate services)
  # ===================================================================

  # Service 1: Spark Consumer (Reads from Kafka, writes to Redis/InfluxDB/ClickHouse)
  spark-consumer:
    build:
      context: ./spark-consumer
      dockerfile: Dockerfile
    container_name: spark-consumer
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
      influxdb:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      producer:
        condition: service_started
    environment:
      # Kafka Configuration
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      KAFKA_TOPIC: "ohlcv-stream"
      KAFKA_STARTING_OFFSETS: "earliest"
      KAFKA_MAX_OFFSETS_PER_TRIGGER: "1000"
      
      # Spark Configuration
      SPARK_APP_NAME: "OHLCV-Spark-Consumer"
      TRIGGER_INTERVAL: "500 milliseconds"
      CHECKPOINT_LOCATION: "/app/checkpoints"
      
      # Logging Configuration
      LOG_DIR: "/app/logs/spark"
      DATA_LOG_DIR: "/app/logs/spark/data"
      LOG_LEVEL: "INFO"
      
      # Redis Configuration
      REDIS_HOST: "redis"
      REDIS_PORT: "6379"
      REDIS_DB: "0"
      REDIS_SOCKET_TIMEOUT: "5"
      
      # InfluxDB Configuration
      INFLUX_URL: "http://influxdb:8086"
      INFLUX_TOKEN: "my-super-secret-auth-token"
      INFLUX_ORG: "my-org"
      INFLUX_BUCKET: "ohlcv_data"
      
      # ClickHouse Configuration
      CLICKHOUSE_HOST: "clickhouse"
      CLICKHOUSE_PORT: "8123"
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DATABASE: "default"
    networks:
      - kafka-network
    volumes:
      - ./logs/spark:/app/logs
      - ./spark-consumer/spark_consumer.py:/app/spark_consumer.py
      - ./spark-consumer/influx_manager.py:/app/influx_manager.py
      - ./spark-consumer/clickhouse_manager.py:/app/clickhouse_manager.py
      - ./spark-consumer/indicator_engine.py:/app/indicator_engine.py
      - spark-checkpoints:/app/checkpoints
    command: >
      spark-submit
      --master local[*]
      --driver-memory 1g
      --executor-memory 1g
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      --conf spark.sql.streaming.checkpointLocation=/app/checkpoints
      --conf spark.streaming.stopGracefullyOnShutdown=true
      /app/spark_consumer.py
    healthcheck:
      test: ["CMD", "/app/healthcheck.sh", "spark-consumer"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

  # Service 2: Signal Processor (Reads from Redis, generates trading signals)
  signal-processor:
    build:
      context: ./spark-consumer
      dockerfile: Dockerfile
    container_name: signal-processor
    depends_on:
      spark-consumer:
        condition: service_started  # Start after spark-consumer starts
      redis:
        condition: service_healthy
      influxdb:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      # Logging Configuration
      LOG_DIR: "/app/logs/signals"
      LOG_LEVEL: "INFO"
      
      # Redis Configuration (Primary data source)
      REDIS_HOST: "redis"
      REDIS_PORT: "6379"
      REDIS_DB: "0"
      REDIS_SOCKET_TIMEOUT: "5"
      
      # InfluxDB Configuration (For historical data if needed)
      INFLUX_URL: "http://influxdb:8086"
      INFLUX_TOKEN: "my-super-secret-auth-token"
      INFLUX_ORG: "my-org"
      INFLUX_BUCKET: "ohlcv_data"
      
      # ClickHouse Configuration (For analytics)
      CLICKHOUSE_HOST: "clickhouse"
      CLICKHOUSE_PORT: "8123"
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DATABASE: "default"
      
      # Signal Processing Configuration
      SIGNAL_CHECK_INTERVAL: "1"  # Check for new data every 1 second
      MIN_DATA_POINTS: "20"  # Minimum data points needed for signal generation
      LOOKBACK_PERIOD: "100"  # Number of candles to look back for analysis
    networks:
      - kafka-network
    volumes:
      - ./logs/signals:/app/logs
      - ./spark-consumer/signal_processor.py:/app/signal_processor.py
      - ./spark-consumer/indicator_engine.py:/app/indicator_engine.py
      - ./spark-consumer/influx_manager.py:/app/influx_manager.py
      - ./spark-consumer/clickhouse_manager.py:/app/clickhouse_manager.py
    command: sh -c "sleep 210 && python /app/signal_processor.py"    # this sleeps for 3.5 mins and starts after spark consumer starts
    healthcheck:
      test: ["CMD", "/app/healthcheck.sh", "signal-processor"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # Service 3: Portfolio Manager (Reads signals, manages portfolio)
  portfolio-manager:
    build:
      context: ./spark-consumer
      dockerfile: Dockerfile
    container_name: portfolio-manager
    depends_on:
      signal-processor:
        condition: service_started  # Start after signal-processor starts
      redis:
        condition: service_healthy
      influxdb:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      # Logging Configuration
      LOG_DIR: "/app/logs/portfolio"
      LOG_LEVEL: "INFO"
      
      # Redis Configuration (Primary data source)
      REDIS_HOST: "redis"
      REDIS_PORT: "6379"
      REDIS_DB: "0"
      REDIS_SOCKET_TIMEOUT: "5"
      
      # InfluxDB Configuration
      INFLUX_URL: "http://influxdb:8086"
      INFLUX_TOKEN: "my-super-secret-auth-token"
      INFLUX_ORG: "my-org"
      INFLUX_BUCKET: "ohlcv_data"
      
      # ClickHouse Configuration
      CLICKHOUSE_HOST: "clickhouse"
      CLICKHOUSE_PORT: "8123"
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DATABASE: "default"
      
      # Portfolio Configuration
      INITIAL_CAPITAL: "100000"  # Starting capital
      MAX_POSITIONS: "10"  # Maximum number of concurrent positions
      POSITION_SIZE_PCT: "10"  # Percentage of capital per position
      RISK_PER_TRADE: "2"  # Maximum risk per trade (%)
      PORTFOLIO_UPDATE_INTERVAL: "5"  # Update portfolio every 5 seconds
    networks:
      - kafka-network
    volumes:
      - ./logs/portfolio:/app/logs
      - ./spark-consumer/portfolio_manager.py:/app/portfolio_manager.py
      - ./spark-consumer/influx_manager.py:/app/influx_manager.py
      - ./spark-consumer/clickhouse_manager.py:/app/clickhouse_manager.py
    command: sh -c "sleep 350 && python /app/portfolio_manager.py"   # this sleeps for 5 mins and then starts
    healthcheck:
      test: ["CMD", "/app/healthcheck.sh", "portfolio-manager"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

networks:
  kafka-network:
    driver: bridge

volumes:
  redis-insight-data:
    driver: local
  redis-data:
    driver: local
  kafka-data:
    driver: local
  spark-checkpoints:
    driver: local
  # Need to comment this out - so that data is not stored locally and clears on restart or every compose up
  influxdb-data:
    driver: local
  clickhouse-data:
    driver: local
    