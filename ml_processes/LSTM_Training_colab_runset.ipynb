{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761294135511,
     "user": {
      "displayName": "walter mitt",
      "userId": "17029525990198959895"
     },
     "user_tz": -330
    },
    "id": "xc9CLmXnhYU2",
    "outputId": "d52ab31c-6fa9-43e9-9141-68634ae1084d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 24 08:22:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   76C    P0             35W /   70W |     106MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7790,
     "status": "ok",
     "timestamp": 1761294146703,
     "user": {
      "displayName": "walter mitt",
      "userId": "17029525990198959895"
     },
     "user_tz": -330
    },
    "id": "FIvcKGyj3SQV",
    "outputId": "fe4cd306-5c33-4e6d-b915-59899dbda2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "✓ GPU Available: /physical_device:GPU:0\n",
      "⚡ Training will be 3-5x faster!\n",
      "✓ Packages ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Check GPU and Install Packages\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"✓ GPU Available: {gpus[0].name}\")\n",
    "    print(\"Training will be 3-5x faster!\")\n",
    "else:\n",
    "    print(\" NO GPU! Go to Runtime → Change runtime type → GPU\")\n",
    "\n",
    "# Install packages (most are pre-installed)\n",
    "!pip install -q scikit-learn pandas matplotlib seaborn joblib\n",
    "print(\"✓ Packages ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4499,
     "status": "ok",
     "timestamp": 1761294151204,
     "user": {
      "displayName": "walter mitt",
      "userId": "17029525990198959895"
     },
     "user_tz": -330
    },
    "id": "UHsKTGKt4puu",
    "outputId": "9a404054-4874-4843-f145-3249b772b927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Mount Google Drive\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1761294219855,
     "user": {
      "displayName": "walter mitt",
      "userId": "17029525990198959895"
     },
     "user_tz": -330
    },
    "id": "jkKz124f420y",
    "outputId": "560544cc-311f-4fb1-a830-bd95593f3f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /content/drive/MyDrive/lstm_project\n",
      "\n",
      "📂 Checking files...\n",
      "✓ lstm_training.py found\n",
      "✓ data/processed found (11 files)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Navigate to Project Directory\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# GOOGLE DRIVE LOCATION\n",
    "PROJECT_PATH = '/content/drive/MyDrive/lstm_project'\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify files are present\n",
    "print(\"\\nChecking files...\")\n",
    "if os.path.exists('lstm_training.py'):\n",
    "    print(\"✓ lstm_training.py found\")\n",
    "else:\n",
    "    print(\"✗ lstm_training.py NOT FOUND!\")\n",
    "\n",
    "if os.path.exists('data/processed'):\n",
    "    files = os.listdir('data/processed')\n",
    "    print(f\"✓ data/processed found ({len(files)} files)\")\n",
    "else:\n",
    "    print(\"✗ data/processed NOT FOUND!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1761294225576,
     "user": {
      "displayName": "walter mitt",
      "userId": "17029525990198959895"
     },
     "user_tz": -330
    },
    "id": "hcy9__it5A4j",
    "outputId": "97bd605f-68ac-4198-e56f-a7c40849bf27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modified script created: lstm_training_50epochs.py\n",
      "  - Epochs: 50\n",
      "  - Early stopping: Effectively disabled (patience=100)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Modify Training Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Read the training script\n",
    "with open('lstm_training.py', 'r') as f:\n",
    "    script = f.read()\n",
    "\n",
    "# Modify CONFIG for 50 epochs without early stopping\n",
    "import re\n",
    "\n",
    "# Set epochs to 50\n",
    "script = re.sub(r\"'epochs':\\s*\\d+\", \"'epochs': 50\", script)\n",
    "\n",
    "# Disable early stopping by setting very high patience\n",
    "script = re.sub(r\"'early_stopping_patience':\\s*\\d+\", \"'early_stopping_patience': 100\", script)\n",
    "\n",
    "# Save modified script\n",
    "with open('lstm_training_50epochs.py', 'w') as f:\n",
    "    f.write(script)\n",
    "\n",
    "print(\"✓ Modified script created: lstm_training_50epochs.py\")\n",
    "print(\"  - Epochs: 50\")\n",
    "print(\"  - Early stopping: Effectively disabled (patience=100)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4971597,
     "status": "ok",
     "timestamp": 1761299201124,
     "user": {
      "displayName": "walter mitt",
      "userId": "17029525990198959895"
     },
     "user_tz": -330
    },
    "id": "jlFcvDCL5HCP",
    "outputId": "dc80dbc2-50b9-4054-b22e-882e90609147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🚀 STARTING TRAINING - 50 EPOCHS WITH GPU\n",
      "======================================================================\n",
      "Estimated time: 25-35 minutes\n",
      "You can close this tab and come back later - training will continue!\n",
      "======================================================================\n",
      "\n",
      "2025-10-24 08:23:54.235825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761294234.390580   34212 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761294234.427093   34212 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761294234.649880   34212 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761294234.649924   34212 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761294234.649929   34212 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761294234.649935   34212 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\n",
      "======================================================================\n",
      "LSTM MODEL TRAINING PIPELINE (IMPROVED)\n",
      "======================================================================\n",
      "Start time: 2025-10-24 08:23:59\n",
      "Expected features: 20 (5 OHLCV + 15 indicators)\n",
      "✓ Found 1 GPU(s)\n",
      "✓ GPU memory growth enabled\n",
      "✓ Output directories created\n",
      "  Run directory: models/run_20251024_082359\n",
      "  Images directory: models/run_20251024_082359/images\n",
      "✓ Log capture initialized: models/run_20251024_082359/training.log\n",
      "\n",
      "► LOADING PREPROCESSED DATA\n",
      "======================================================================\n",
      "✓ Data arrays loaded successfully\n",
      "  X_train shape: (385795, 60, 20)\n",
      "  X_val shape: (82670, 60, 20)\n",
      "  X_test shape: (82672, 60, 20)\n",
      "\n",
      "  Data format:\n",
      "    Sequence length: 60\n",
      "    Number of features: 20\n",
      "  ✓ Feature count matches expected: 20\n",
      "✓ Scalers loaded (features + target)\n",
      "\n",
      "  Validating feature columns...\n",
      "  Expected features: 20\n",
      "  Loaded features: 20\n",
      "  ✓ All features validated correctly\n",
      "\n",
      "  Features being used:\n",
      "     1. open\n",
      "     2. high\n",
      "     3. low\n",
      "     4. close\n",
      "     5. volume\n",
      "     6. sma_5\n",
      "     7. sma_10\n",
      "     8. ema_7\n",
      "     9. ema_11\n",
      "    10. macd_line\n",
      "    11. macd_signal\n",
      "    12. atr_14\n",
      "    13. momentum\n",
      "    14. bb_upper\n",
      "    15. bb_lower\n",
      "    16. vma_10\n",
      "    17. volume_ratio\n",
      "    18. rsi_14\n",
      "    19. price_change\n",
      "    20. price_change_pct\n",
      "\n",
      "  Preprocessing configuration:\n",
      "    Prediction horizon: 1\n",
      "    Label type: regression\n",
      "    Sequence length: 60\n",
      "\n",
      "  Data Summary:\n",
      "    Training samples: 385,795\n",
      "    Validation samples: 82,670\n",
      "    Test samples: 82,672\n",
      "    Total samples: 551,137\n",
      "\n",
      "► BUILDING MODEL\n",
      "======================================================================\n",
      "Model type: stacked_lstm\n",
      "Input shape: (60, 20)\n",
      "LSTM units: [128, 64, 32]\n",
      "Dense units: [16]\n",
      "Dropout rate: 0.2\n",
      "Recurrent dropout: 0\n",
      "I0000 00:00:1761294268.920842   34212 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13838 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "\n",
      "✓ Model compiled successfully\n",
      "  Optimizer: Adam (lr=0.001)\n",
      "  Loss: mse\n",
      "  Total parameters: 138,657\n",
      "\n",
      "Model Architecture:\n",
      "Model: \"sequential\"\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ lstm (LSTM)                     │ (None, 60, 128)        │        76,288 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout (Dropout)               │ (None, 60, 128)        │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ lstm_1 (LSTM)                   │ (None, 60, 64)         │        49,408 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_1 (Dropout)             │ (None, 60, 64)         │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ lstm_2 (LSTM)                   │ (None, 32)             │        12,416 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_2 (Dropout)             │ (None, 32)             │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (Dense)                   │ (None, 16)             │           528 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dropout_3 (Dropout)             │ (None, 16)             │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense_1 (Dense)                 │ (None, 1)              │            17 │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      " Total params: 138,657 (541.63 KB)\n",
      " Trainable params: 138,657 (541.63 KB)\n",
      " Non-trainable params: 0 (0.00 B)\n",
      "\n",
      "► TRAINING MODEL\n",
      "======================================================================\n",
      "Batch size: 64\n",
      "Max epochs: 50\n",
      "\n",
      "Starting training...\n",
      "✓ Callbacks created\n",
      "  Early stopping patience: 100\n",
      "  Reduce LR patience: 7\n",
      "  Model checkpoint: models/run_20251024_082359/best_model.h5\n",
      "2025-10-24 08:24:35.110733: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1851816000 exceeds 10% of free system memory.\n",
      "2025-10-24 08:24:37.416564: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1851816000 exceeds 10% of free system memory.\n",
      "Epoch 1/50\n",
      "I0000 00:00:1761294285.556591   34399 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0699 - mae: 0.1310 - mse: 0.06992025-10-24 08:26:16.918306: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 396816000 exceeds 10% of free system memory.\n",
      "2025-10-24 08:26:17.589467: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 396816000 exceeds 10% of free system memory.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00015, saving model to models/run_20251024_082359/best_model.h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 16ms/step - loss: 0.0699 - mae: 0.1310 - mse: 0.0699 - val_loss: 1.4826e-04 - val_mae: 0.0107 - val_mse: 1.4826e-04 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0476 - mae: 0.1215 - mse: 0.0476\n",
      "Epoch 2: val_loss did not improve from 0.00015\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 16ms/step - loss: 0.0476 - mae: 0.1215 - mse: 0.0476 - val_loss: 5.4279e-04 - val_mae: 0.0211 - val_mse: 5.4279e-04 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0433 - mae: 0.1150 - mse: 0.0433\n",
      "Epoch 3: val_loss did not improve from 0.00015\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0433 - mae: 0.1150 - mse: 0.0433 - val_loss: 7.6276e-04 - val_mae: 0.0269 - val_mse: 7.6276e-04 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0436 - mae: 0.1137 - mse: 0.0436\n",
      "Epoch 4: val_loss did not improve from 0.00015\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 16ms/step - loss: 0.0436 - mae: 0.1137 - mse: 0.0436 - val_loss: 4.0088e-04 - val_mae: 0.0193 - val_mse: 4.0088e-04 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0423 - mae: 0.1122 - mse: 0.0423\n",
      "Epoch 5: val_loss improved from 0.00015 to 0.00002, saving model to models/run_20251024_082359/best_model.h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 16ms/step - loss: 0.0423 - mae: 0.1122 - mse: 0.0423 - val_loss: 1.8919e-05 - val_mae: 0.0033 - val_mse: 1.8919e-05 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0425 - mae: 0.1125 - mse: 0.0425\n",
      "Epoch 6: val_loss did not improve from 0.00002\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 16ms/step - loss: 0.0425 - mae: 0.1125 - mse: 0.0425 - val_loss: 2.3756e-04 - val_mae: 0.0143 - val_mse: 2.3756e-04 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m6025/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0421 - mae: 0.1119 - mse: 0.0421\n",
      "Epoch 7: val_loss did not improve from 0.00002\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 16ms/step - loss: 0.0421 - mae: 0.1119 - mse: 0.0421 - val_loss: 1.3543e-04 - val_mae: 0.0104 - val_mse: 1.3543e-04 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0427 - mae: 0.1128 - mse: 0.0427\n",
      "Epoch 8: val_loss did not improve from 0.00002\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0427 - mae: 0.1128 - mse: 0.0427 - val_loss: 2.7014e-04 - val_mae: 0.0161 - val_mse: 2.7014e-04 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0430 - mae: 0.1135 - mse: 0.0430\n",
      "Epoch 9: val_loss did not improve from 0.00002\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 16ms/step - loss: 0.0430 - mae: 0.1135 - mse: 0.0430 - val_loss: 3.4610e-04 - val_mae: 0.0179 - val_mse: 3.4610e-04 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m6025/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0422 - mae: 0.1129 - mse: 0.0422\n",
      "Epoch 10: val_loss did not improve from 0.00002\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0422 - mae: 0.1129 - mse: 0.0422 - val_loss: 2.0342e-04 - val_mae: 0.0137 - val_mse: 2.0342e-04 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0426 - mae: 0.1131 - mse: 0.0426\n",
      "Epoch 11: val_loss did not improve from 0.00002\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0426 - mae: 0.1131 - mse: 0.0426 - val_loss: 2.1616e-04 - val_mae: 0.0144 - val_mse: 2.1616e-04 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0428 - mae: 0.1134 - mse: 0.0428\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.00002\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 16ms/step - loss: 0.0428 - mae: 0.1134 - mse: 0.0428 - val_loss: 5.3083e-04 - val_mae: 0.0223 - val_mse: 5.3083e-04 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0422 - mae: 0.1132 - mse: 0.0422\n",
      "Epoch 13: val_loss improved from 0.00002 to 0.00001, saving model to models/run_20251024_082359/best_model.h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 15ms/step - loss: 0.0422 - mae: 0.1132 - mse: 0.0422 - val_loss: 1.4504e-05 - val_mae: 0.0030 - val_mse: 1.4504e-05 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0419 - mae: 0.1125 - mse: 0.0419\n",
      "Epoch 14: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0419 - mae: 0.1125 - mse: 0.0419 - val_loss: 4.5779e-04 - val_mae: 0.0212 - val_mse: 4.5779e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0423 - mae: 0.1131 - mse: 0.0423\n",
      "Epoch 15: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0423 - mae: 0.1131 - mse: 0.0423 - val_loss: 3.5907e-05 - val_mae: 0.0052 - val_mse: 3.5907e-05 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0420 - mae: 0.1125 - mse: 0.0420\n",
      "Epoch 16: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 15ms/step - loss: 0.0420 - mae: 0.1125 - mse: 0.0420 - val_loss: 3.3306e-05 - val_mae: 0.0052 - val_mse: 3.3306e-05 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m6025/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0415 - mae: 0.1122 - mse: 0.0415\n",
      "Epoch 17: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0415 - mae: 0.1122 - mse: 0.0415 - val_loss: 2.7220e-04 - val_mae: 0.0163 - val_mse: 2.7220e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0417 - mae: 0.1124 - mse: 0.0417\n",
      "Epoch 18: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 16ms/step - loss: 0.0417 - mae: 0.1124 - mse: 0.0417 - val_loss: 1.8734e-04 - val_mae: 0.0132 - val_mse: 1.8734e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0420 - mae: 0.1132 - mse: 0.0420\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0420 - mae: 0.1132 - mse: 0.0420 - val_loss: 2.0720e-05 - val_mae: 0.0035 - val_mse: 2.0720e-05 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0425 - mae: 0.1134 - mse: 0.0425\n",
      "Epoch 20: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0425 - mae: 0.1134 - mse: 0.0425 - val_loss: 2.2274e-04 - val_mae: 0.0144 - val_mse: 2.2274e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0416 - mae: 0.1125 - mse: 0.0416\n",
      "Epoch 21: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0416 - mae: 0.1125 - mse: 0.0416 - val_loss: 4.7779e-05 - val_mae: 0.0066 - val_mse: 4.7779e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0418 - mae: 0.1126 - mse: 0.0418\n",
      "Epoch 22: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0418 - mae: 0.1126 - mse: 0.0418 - val_loss: 1.3530e-04 - val_mae: 0.0112 - val_mse: 1.3530e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0414 - mae: 0.1124 - mse: 0.0414\n",
      "Epoch 23: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0414 - mae: 0.1124 - mse: 0.0414 - val_loss: 1.1673e-04 - val_mae: 0.0103 - val_mse: 1.1673e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m6028/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0414 - mae: 0.1124 - mse: 0.0414\n",
      "Epoch 24: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0414 - mae: 0.1124 - mse: 0.0414 - val_loss: 3.6009e-04 - val_mae: 0.0185 - val_mse: 3.6009e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0422 - mae: 0.1127 - mse: 0.0422\n",
      "Epoch 25: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0422 - mae: 0.1127 - mse: 0.0422 - val_loss: 1.9221e-04 - val_mae: 0.0137 - val_mse: 1.9221e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m6028/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0414 - mae: 0.1124 - mse: 0.0414\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0414 - mae: 0.1124 - mse: 0.0414 - val_loss: 3.2709e-04 - val_mae: 0.0171 - val_mse: 3.2709e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0415 - mae: 0.1124 - mse: 0.0415\n",
      "Epoch 27: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0415 - mae: 0.1124 - mse: 0.0415 - val_loss: 1.5659e-04 - val_mae: 0.0118 - val_mse: 1.5659e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m6025/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0418 - mae: 0.1128 - mse: 0.0418\n",
      "Epoch 28: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0418 - mae: 0.1128 - mse: 0.0418 - val_loss: 5.1172e-05 - val_mae: 0.0067 - val_mse: 5.1172e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0417 - mae: 0.1127 - mse: 0.0417\n",
      "Epoch 29: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 15ms/step - loss: 0.0417 - mae: 0.1127 - mse: 0.0417 - val_loss: 1.6721e-04 - val_mae: 0.0127 - val_mse: 1.6721e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0415 - mae: 0.1127 - mse: 0.0415\n",
      "Epoch 30: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0415 - mae: 0.1127 - mse: 0.0415 - val_loss: 1.9848e-04 - val_mae: 0.0139 - val_mse: 1.9848e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0421 - mae: 0.1132 - mse: 0.0421\n",
      "Epoch 31: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 15ms/step - loss: 0.0421 - mae: 0.1132 - mse: 0.0421 - val_loss: 1.4830e-04 - val_mae: 0.0120 - val_mse: 1.4830e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0417 - mae: 0.1130 - mse: 0.0417\n",
      "Epoch 32: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 15ms/step - loss: 0.0417 - mae: 0.1130 - mse: 0.0417 - val_loss: 2.7799e-05 - val_mae: 0.0042 - val_mse: 2.7799e-05 - learning_rate: 1.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m6028/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0418 - mae: 0.1127 - mse: 0.0418\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 16ms/step - loss: 0.0418 - mae: 0.1127 - mse: 0.0418 - val_loss: 1.3521e-04 - val_mae: 0.0114 - val_mse: 1.3521e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m6028/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0417 - mae: 0.1129 - mse: 0.0417\n",
      "Epoch 34: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0417 - mae: 0.1129 - mse: 0.0417 - val_loss: 1.3408e-04 - val_mae: 0.0112 - val_mse: 1.3408e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0411 - mae: 0.1125 - mse: 0.0411\n",
      "Epoch 35: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 16ms/step - loss: 0.0411 - mae: 0.1125 - mse: 0.0411 - val_loss: 1.1228e-04 - val_mae: 0.0103 - val_mse: 1.1228e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0413 - mae: 0.1126 - mse: 0.0413\n",
      "Epoch 36: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0413 - mae: 0.1126 - mse: 0.0413 - val_loss: 9.9064e-05 - val_mae: 0.0098 - val_mse: 9.9064e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0417 - mae: 0.1128 - mse: 0.0417\n",
      "Epoch 37: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 15ms/step - loss: 0.0417 - mae: 0.1128 - mse: 0.0417 - val_loss: 1.6789e-04 - val_mae: 0.0128 - val_mse: 1.6789e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0420 - mae: 0.1133 - mse: 0.0420\n",
      "Epoch 38: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0420 - mae: 0.1133 - mse: 0.0420 - val_loss: 5.9411e-05 - val_mae: 0.0074 - val_mse: 5.9411e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0421 - mae: 0.1131 - mse: 0.0421\n",
      "Epoch 39: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0421 - mae: 0.1131 - mse: 0.0421 - val_loss: 1.3410e-04 - val_mae: 0.0114 - val_mse: 1.3410e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0416 - mae: 0.1123 - mse: 0.0416\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0416 - mae: 0.1123 - mse: 0.0416 - val_loss: 1.0417e-04 - val_mae: 0.0100 - val_mse: 1.0417e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0418 - mae: 0.1129 - mse: 0.0418\n",
      "Epoch 41: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0418 - mae: 0.1129 - mse: 0.0418 - val_loss: 9.6745e-05 - val_mae: 0.0097 - val_mse: 9.6745e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m6025/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0416 - mae: 0.1128 - mse: 0.0416\n",
      "Epoch 42: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 15ms/step - loss: 0.0416 - mae: 0.1128 - mse: 0.0416 - val_loss: 2.0050e-04 - val_mae: 0.0138 - val_mse: 2.0050e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m6025/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0421 - mae: 0.1133 - mse: 0.0421\n",
      "Epoch 43: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0421 - mae: 0.1133 - mse: 0.0421 - val_loss: 6.8590e-05 - val_mae: 0.0080 - val_mse: 6.8590e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m6028/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0410 - mae: 0.1123 - mse: 0.0410\n",
      "Epoch 44: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 16ms/step - loss: 0.0410 - mae: 0.1123 - mse: 0.0410 - val_loss: 1.2592e-04 - val_mae: 0.0109 - val_mse: 1.2592e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0414 - mae: 0.1127 - mse: 0.0414\n",
      "Epoch 45: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 16ms/step - loss: 0.0414 - mae: 0.1127 - mse: 0.0414 - val_loss: 1.5145e-04 - val_mae: 0.0120 - val_mse: 1.5145e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m6027/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0417 - mae: 0.1130 - mse: 0.0417\n",
      "Epoch 46: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 16ms/step - loss: 0.0417 - mae: 0.1130 - mse: 0.0417 - val_loss: 9.6210e-05 - val_mae: 0.0096 - val_mse: 9.6210e-05 - learning_rate: 3.1250e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0419 - mae: 0.1132 - mse: 0.0419\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 15ms/step - loss: 0.0419 - mae: 0.1132 - mse: 0.0419 - val_loss: 1.0012e-04 - val_mae: 0.0098 - val_mse: 1.0012e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m6026/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0414 - mae: 0.1126 - mse: 0.0414\n",
      "Epoch 48: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 15ms/step - loss: 0.0414 - mae: 0.1126 - mse: 0.0414 - val_loss: 1.2091e-04 - val_mae: 0.0107 - val_mse: 1.2091e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0415 - mae: 0.1127 - mse: 0.0415\n",
      "Epoch 49: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0415 - mae: 0.1127 - mse: 0.0415 - val_loss: 1.4714e-04 - val_mae: 0.0119 - val_mse: 1.4714e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m6025/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0421 - mae: 0.1131 - mse: 0.0421\n",
      "Epoch 50: val_loss did not improve from 0.00001\n",
      "\u001b[1m6029/6029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0421 - mae: 0.1131 - mse: 0.0421 - val_loss: 1.5457e-04 - val_mae: 0.0122 - val_mse: 1.5457e-04 - learning_rate: 1.5625e-05\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "✓ Training complete\n",
      "  Total time: 4903.91 seconds (81.73 minutes)\n",
      "  Epochs trained: 50\n",
      "  Best val_loss: 0.000015\n",
      "\n",
      "► EVALUATING MODEL\n",
      "======================================================================\n",
      "2025-10-24 09:46:15.026468: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 396825600 exceeds 10% of free system memory.\n",
      "✓ Predictions denormalized using target scaler\n",
      "\n",
      "✓ Evaluation complete\n",
      "\n",
      "Test Set Metrics:\n",
      "  RMSE: 35.302760\n",
      "  MAE: 29.531633\n",
      "  MAPE: 5.09%\n",
      "  R² Score: 0.829740\n",
      "\n",
      "Error Statistics:\n",
      "  Mean Error: 1.090567\n",
      "  Std Error: 35.285911\n",
      "  Max Error: 148.728369\n",
      "  Min Error: 0.000562\n",
      "\n",
      "► GENERATING PLOTS\n",
      "======================================================================\n",
      "✓ Training history plot saved: models/run_20251024_082359/images/training_history.png\n",
      "✓ Predictions plot saved: models/run_20251024_082359/images/predictions_analysis.png\n",
      "✓ Error analysis plot saved: models/run_20251024_082359/images/error_analysis.png\n",
      "\n",
      "► SAVING MODEL AND RESULTS\n",
      "======================================================================\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "✓ Full model saved: models/run_20251024_082359/final_model.h5\n",
      "✓ Model architecture saved: models/run_20251024_082359/model_architecture.json\n",
      "✓ Model weights saved: models/run_20251024_082359/model_weights.weights.h5\n",
      "✓ Model summary saved: models/run_20251024_082359/model_summary.txt\n",
      "✓ Test metrics saved: models/run_20251024_082359/test_metrics.csv\n",
      "✓ Feature columns saved: models/run_20251024_082359/features_used.csv\n",
      "✓ Training configuration saved: models/run_20251024_082359/training_config.csv\n",
      "✓ Full training history saved: models/run_20251024_082359/full_training_history.csv\n",
      "\n",
      "✓ All files saved successfully!\n",
      "\n",
      "► GENERATING MARKDOWN REPORT\n",
      "======================================================================\n",
      "✓ Markdown report saved: models/run_20251024_082359/TRAINING_REPORT.md\n",
      "\n",
      "✓ TRAINING COMPLETE\n",
      "======================================================================\n",
      "Total time: 4958.07 seconds (82.63 minutes)\n",
      "Output directory: models/run_20251024_082359\n",
      "Markdown report: models/run_20251024_082359/TRAINING_REPORT.md\n",
      "\n",
      "Model Performance Summary:\n",
      "  RMSE: 35.302760\n",
      "  MAE: 29.531633\n",
      "  MAPE: 5.09%\n",
      "  R² Score: 0.829740\n",
      "\n",
      "Features used: 20\n",
      "  - OHLCV: 5 features\n",
      "  - Technical Indicators: 15 features\n",
      "\n",
      "All outputs saved to: models/run_20251024_082359\n",
      "  - Training log: training.log\n",
      "  - Markdown report: TRAINING_REPORT.md\n",
      "  - Images: images/\n",
      "\n",
      "Advanced Usage:\n",
      "  - For confidence intervals: call make_predictions_with_confidence(model, X_test)\n",
      "\n",
      "Next steps:\n",
      "  1. Open TRAINING_REPORT.md for a comprehensive summary\n",
      "  2. Review visualizations in the images/ folder\n",
      "  3. Check training.log for complete console output\n",
      "  4. Adjust hyperparameters in CONFIG if needed\n",
      "  5. Use the trained model for predictions\n",
      "======================================================================\n",
      "\n",
      "✓ All outputs saved successfully!\n",
      "📊 View complete report: models/run_20251024_082359/TRAINING_REPORT.md\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: RUN TRAINING (This will take ~30 minutes)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING - 50 EPOCHS WITH GPU\")\n",
    "print(\"=\"*70)\n",
    "print(\"Estimated time: 25-35 minutes\")\n",
    "print(\"You can close this tab and come back later - training will continue!\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run the training script\n",
    "!python lstm_training_50epochs.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOKZ09KuKZdDWezHHZ8Woai",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
