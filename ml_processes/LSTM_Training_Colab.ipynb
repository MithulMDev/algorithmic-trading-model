{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ LSTM Stock Prediction Training\n",
    "## Complete Training Pipeline\n",
    "\n",
    "**Before running this notebook:**\n",
    "1. Create folder `stock_lstm_project` in your Google Drive\n",
    "2. Inside it, create: `data/`, `scripts/`, `outputs/`\n",
    "3. Upload training script to `scripts/lstm_training.py`\n",
    "4. Upload data files to `data/` folder:\n",
    "   - train_sequences.npz\n",
    "   - val_sequences.npz\n",
    "   - test_sequences.npz\n",
    "   - scalers.pkl\n",
    "\n",
    "**Recommended Runtime:** GPU (T4)\n",
    "- Go to: **Runtime ‚Üí Change runtime type ‚Üí GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Step 2: Setup Project Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to project directory\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/stock_lstm_project'\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(f\"üìÇ Current directory: {os.getcwd()}\")\n",
    "print(\"\\nüìÅ Project structure:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for root, dirs, files in os.walk('.', topdown=True):\n",
    "    # Limit depth to 2 levels\n",
    "    level = root.replace('.', '').count(os.sep)\n",
    "    if level < 3:\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f'{indent}üìÅ {os.path.basename(root)}/')\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            if not file.startswith('.'):\n",
    "                print(f'{subindent}üìÑ {file}')\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]  # Skip hidden dirs\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Step 3: Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all required data files exist\n",
    "data_dir = 'data'\n",
    "required_files = [\n",
    "    'train_sequences.npz',\n",
    "    'val_sequences.npz', \n",
    "    'test_sequences.npz',\n",
    "    'scalers.pkl'\n",
    "]\n",
    "\n",
    "print(\"üîç Checking required data files...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_present = True\n",
    "for file in required_files:\n",
    "    filepath = os.path.join(data_dir, file)\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    size = \"\"\n",
    "    if exists:\n",
    "        size_mb = os.path.getsize(filepath) / (1024*1024)\n",
    "        size = f\"({size_mb:.2f} MB)\"\n",
    "    print(f\"{status} {file:25} {'Found' if exists else 'MISSING':10} {size}\")\n",
    "    if not exists:\n",
    "        all_present = False\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\n‚úÖ All data files present! Ready to train.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR: Some files are missing!\")\n",
    "    print(\"Please upload them to the data/ folder before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Step 4: Install/Verify Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (most are pre-installed in Colab)\n",
    "!pip install -q tensorflow pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "# Verify installations\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ All packages installed and imported successfully!\")\n",
    "print(f\"\\nüìä Package Versions:\")\n",
    "print(f\"   TensorFlow: {tf.__version__}\")\n",
    "print(f\"   Keras:      {tf.keras.__version__}\")\n",
    "print(f\"   NumPy:      {np.__version__}\")\n",
    "print(f\"   Pandas:     {pd.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"\\nüñ•Ô∏è  GPU Status:\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"   ‚úÖ {len(gpus)} GPU(s) available\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"      - {gpu.name}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  No GPU detected - training on CPU\")\n",
    "    print(f\"   üí° To enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Step 5: RUN TRAINING!\n",
    "\n",
    "This cell will:\n",
    "1. Load all preprocessed data\n",
    "2. Build LSTM model\n",
    "3. Train the model (with early stopping)\n",
    "4. Evaluate on test set\n",
    "5. Generate visualizations\n",
    "6. Save model and results\n",
    "\n",
    "**Expected training time:**\n",
    "- With GPU: 10-30 minutes\n",
    "- Without GPU: 1-3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to data directory (where the script expects files)\n",
    "os.chdir(f'{PROJECT_ROOT}/data')\n",
    "\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ STARTING TRAINING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚è≥ This may take 10-30 minutes with GPU...\\n\")\n",
    "\n",
    "# Run the training script\n",
    "%run ../scripts/lstm_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Step 6: View Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Ensure we're in the right directory\n",
    "os.chdir(f'{PROJECT_ROOT}/data')\n",
    "\n",
    "plot_dir = 'plots'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä TRAINING RESULTS VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training History\n",
    "print(\"\\nüìà 1. Training History (Loss & MAE)\")\n",
    "print(\"-\" * 80)\n",
    "if os.path.exists(f'{plot_dir}/training_history.png'):\n",
    "    display(Image(f'{plot_dir}/training_history.png'))\n",
    "else:\n",
    "    print(\"‚ùå Plot not found!\")\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nüìà 2. Predicted vs Actual Values\")\n",
    "print(\"-\" * 80)\n",
    "if os.path.exists(f'{plot_dir}/predictions.png'):\n",
    "    display(Image(f'{plot_dir}/predictions.png'))\n",
    "else:\n",
    "    print(\"‚ùå Plot not found!\")\n",
    "\n",
    "# Error Distribution\n",
    "print(\"\\nüìâ 3. Error Distribution Analysis\")\n",
    "print(\"-\" * 80)\n",
    "if os.path.exists(f'{plot_dir}/error_distribution.png'):\n",
    "    display(Image(f'{plot_dir}/error_distribution.png'))\n",
    "else:\n",
    "    print(\"‚ùå Plot not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Step 7: Load and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Ensure we're in data directory\n",
    "os.chdir(f'{PROJECT_ROOT}/data')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç LOADING TRAINED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'final_lstm_model.keras'\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded successfully from: {model_path}\")\n",
    "print(f\"\\nüìã Model Architecture:\")\n",
    "print(\"-\" * 80)\n",
    "model.summary()\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load scalers\n",
    "with open('scalers.pkl', 'rb') as f:\n",
    "    scalers = pickle.load(f)\n",
    "print(f\"\\n‚úÖ Scalers loaded successfully\")\n",
    "\n",
    "# Load test data\n",
    "test_data = np.load('test_sequences.npz')\n",
    "X_test = test_data['X']\n",
    "y_test = test_data['y']\n",
    "\n",
    "print(f\"‚úÖ Test data loaded: {X_test.shape[0]:,} sequences\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Step 8: Make Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make predictions on first 10 test samples\n",
    "num_samples = 10\n",
    "predictions = model.predict(X_test[:num_samples])\n",
    "\n",
    "print(f\"\\nüìä First {num_samples} Predictions:\\n\")\n",
    "print(f\"{'Sample':>8} | {'Predicted':>12} | {'Actual':>12} | {'Error':>12} | {'Error %':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    pred = predictions[i][0]\n",
    "    actual = y_test[i]\n",
    "    error = pred - actual\n",
    "    error_pct = (error / (abs(actual) + 1e-8)) * 100\n",
    "    \n",
    "    print(f\"{i+1:>8} | {pred:>12.4f} | {actual:>12.4f} | {error:>12.4f} | {error_pct:>9.2f}%\")\n",
    "\n",
    "# Calculate metrics on these samples\n",
    "mae = np.mean(np.abs(predictions.flatten()[:num_samples] - y_test[:num_samples]))\n",
    "mse = np.mean((predictions.flatten()[:num_samples] - y_test[:num_samples])**2)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\nüìà Metrics for these {num_samples} samples:\")\n",
    "print(f\"   MAE:  {mae:.6f}\")\n",
    "print(f\"   MSE:  {mse:.6f}\")\n",
    "print(f\"   RMSE: {rmse:.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• Step 9: Download Model (Optional)\n",
    "\n",
    "If you want to download the model to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.chdir(f'{PROJECT_ROOT}/data')\n",
    "\n",
    "print(\"üì• Downloading model files...\\n\")\n",
    "\n",
    "# Download model\n",
    "print(\"Downloading: final_lstm_model.keras\")\n",
    "files.download('final_lstm_model.keras')\n",
    "\n",
    "# Download scalers\n",
    "print(\"Downloading: scalers.pkl\")\n",
    "files.download('scalers.pkl')\n",
    "\n",
    "# Download training history\n",
    "print(\"Downloading: training_history.pkl\")\n",
    "files.download('training_history.pkl')\n",
    "\n",
    "print(\"\\n‚úÖ Downloads complete!\")\n",
    "print(\"\\nüí° These files are also saved in your Google Drive at:\")\n",
    "print(f\"   {PROJECT_ROOT}/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Summary\n",
    "\n",
    "### ‚úÖ What was created:\n",
    "1. **Models:**\n",
    "   - `best_lstm_model.keras` - Best model during training\n",
    "   - `final_lstm_model.keras` - Final trained model\n",
    "\n",
    "2. **Data:**\n",
    "   - `training_history.pkl` - Complete training history\n",
    "\n",
    "3. **Visualizations:**\n",
    "   - `plots/training_history.png` - Loss curves\n",
    "   - `plots/predictions.png` - Prediction analysis\n",
    "   - `plots/error_distribution.png` - Error analysis\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. Analyze the plots to understand model performance\n",
    "2. If metrics are good, use model for real predictions\n",
    "3. If metrics are poor, consider:\n",
    "   - Collecting more data\n",
    "   - Adjusting model architecture\n",
    "   - Tuning hyperparameters\n",
    "   - Adding more features\n",
    "\n",
    "### üíæ All files are saved in Google Drive:\n",
    "`/content/drive/MyDrive/stock_lstm_project/data/`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "LSTM_Stock_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
